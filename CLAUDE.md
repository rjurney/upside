# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Goal

See @ASSIGNENT.md for the project goal and requirements.

## Claude Psychology

Do not be afraid to question what I say. Do not always respond with "You're right!" Question the assertions I make and decide whether they are true. If they are probably true, don't question them. If they are probably false, question them. If you are unsure, question them. Always think critically about what I say and decide for yourself whether it is true or false.

## Commands

### Development

- Install Dependencies: `poetry install`
- Run CLI: `poetry run upside`
- Build/Generate upside/baml_client code: `baml-cli generate`
- Test baml_src code: `baml-cli test`
- Test all: `poetry run pytest tests/`
- Test single: `poetry run pytest tests/path_to_test.py::test_name`
- Lint: `pre-commit run` is better than `poetry run flake8`. Don't use `--all-files` with pre-commit.
- Format: `poetry run black upside tests`, `poetry run isort upside tests`
- Type check: `poetry run zuban check`
- Place temporary scripts for debugging in the `scripts/` directory.

### Testing Entity Resolution

Test one iteration of the full email parsing pipeline each time you make changes to entity resolution code:

- Parse with `upside parse`

### Common Workflows

- Download emails: `upside download`
- Parse emails: `upside parse`
- Build KG: `upside kg`

## Architecture Overview

### Project Structure

- **upside/** - Core application code
  - **download/** - Download code for Kaggle Enron dataset
  - **parse/** - Email parsing logic
  - **kg/** - Knowledge graph building logic
- **tests/** - Test suite

### Key Technologies

- **Programming Language**: Python 3.12
- **Email Format: [Kaggle CSV of Enron emails](https://www.kaggle.com/datasets/wcukierski/enron-email-dataset/data)**
- **LLM Integration**: BAML (Boundary AI Markup Language) for structured extraction
- **Data Processing**: Apache Spark (PySpark) for ETL and graph operations

### Data Flow

1. **Downloading**: Download emails â†’ CSV files using Kaggle API [kaggle](https://pypi.org/project/kaggle/)
2. **Parsing**: Extract threads via PyPi package [threading](https://pypi.org/project/threading/) from CSV to structured Parquet files
3. Figure out what to do next depending on how much time we have :)

## Code Style

- KISS: KEEP IT SIMPLE STUPID. Do not over-engineer solutions. ESPECIALLY for Spark / PySpark.
- Line length: 100 characters
- Python version: 3.12
- Formatter: black with isort (profile=black)
- Types: Always use type annotations, warn on any return
- Imports: Use absolute imports, organize imports to be PEP compliant with isort (profile=black)
- Error handling: Use specific exception types with logging
- Naming: snake_case for variables/functions, CamelCase for classes
- BAML: Use for LLM-related code, regenerate client with `baml-cli generate`
- Whitespaces: leave no trailing whitespaces, use 4 spaces for indentation, leave no whitespace on blank lines
- Blank lines: Do not indent any blank lines in Python files. Indent should be 0 for these lines. Indent to 0 spaces when replacing a line with a blank line.
- Strings: Use double quotes for strings, use f-strings for string interpolation
- Docstrings: Use Numpy style for docstrings, include type hints in docstrings
- Comments: Use comments to explain complex code, avoid obvious comments
- Tests: Use pytest for testing, include type hints in test functions, use fixtures for setup/teardown
- Tests: Don't make a class to contain unit tests. Just write the tests in pytest style.
- Type hints: Use Python 3.9 type hints for all function parameters and return types. Use `list`, `dict`, `tuple`, etc. instead of `List`, `Dict`, `Tuple` from the `typing` module. Use `Optional` from the `typing` module for optional parameters.
- Type checking: Use zuban for type checking, run zuban before committing code
- Documentation: Use Sphinx for documentation, include docstrings in all public functions/classes
- Code style: Follow PEP 8 for Python code style, use flake8 for linting
- Zuban: Use zuban for type checking, it is a faster version of mypy. Run zuban via pre-commit before committing code. Configure it in `pyproject.toml` under `[tool.zuban]`.
- Pre-commit: Use pre-commit for linting and formatting, configure it in `.pre-commit-config.yaml`
- Git: Use git for version control, commit often with clear messages, use branches for new features/bug fixes. Always test new features in the CLI before you commit them.
- Poetry: Use poetry for dependency management and packaging, configure it in `pyproject.toml`
- discord.py package - always use selective imports for `discord` - YES `from discord import x` - NO `import discord`
- Imports - always import up top in PEP8 format. Do not import inside functions or classes. Use absolute imports, not relative imports. Do not use wildcard imports (e.g., `from module import *`). Always import specific classes or functions from modules.
- Submodules - submodules go under `subs/`. Ignore them completely. Never write to submodules in anything you do.
- Space Lines - never create a line with only spaces.
- Imports - don't check if things are installed and handle it with a try/except. Instead, assume they are installed and import them directly. If they are not installed, the code will fail at runtime, which is acceptable in this project.

## File Formats

Always save data in two formats:

1. Parquet - for efficient processing with Spark and Pandas without processing sub-folders manually.
2. Single file JSON Lines - for debugging and manual inspection.

When writing code that outputs data, ensure both formats are written unless explicitly instructed otherwise.

## Development Guidelines

- Command Line Interfaces - at the end of your coding tasks, please alter the 'upside' CLI to accommodate the changes. It is a Python / Click CLI.
- Separate logic from the CLI - separate the logic under `upside` and sub-modules from the command line interface (CLI) code in `upside.cli`. The CLI should only handle input/output from/to the user and should not contain any business logic. For example the module for `upside kg` should be in `upside.kg.*` and not in `upside.cli.kg`.
- Help strings - never put the default option values in the help strings. The help strings should only describe what the option does, not what the default value is. The default values are already documented in the `config.yml` file and will be printed via the `@click.command(context_settings={"show_default": True})` decorator of each Click command.
- Read the README - consult the README before taking action. The README contains information about the project and how to use it. If you need to add a new command or change an existing one, consult the README first.
- Update the README - if appropriate, update the README with any new commands or changes to existing commands. The README should always reflect the current state of the project.
- Use Poetry - use poetry for dependency management and packaging. Do not use pip or conda.
- Use BAML - use BAML for LLM-related code. Do not use any other libraries or frameworks for LLM-related code. BAML is an extension of Jinja2 and is used for templating LLM information extraction in this project. Use BAML to generate code for the BAML client and to process data.
- DO NOT WRITE TO the `upside.baml_client` directory. This directory is generated by the `baml-cli generate` command and should not be modified directly. Instead, use the `baml-cli generate` command to regenerate the client when needed.
- Use PySpark for ETL - use PySpark for ETL and batch data processing to build our knowledge graph. Do not use any other libraries or frameworks for data processing. Use PySpark to take the output of our BAML client and transform it into a knowledge graph.
- PySpark - Do not break up dataflow into functions for loading, computing this, computing that, etc. Create a single function that performs the entire dataflow at hand. Do not check if columns exist, assume they do. Do not check if paths exist, assume they do. We prefer a more linear flow for Spark scripts and simple code over complexity. This only applies to Spark code.
- PySpark - assume the fields are present, don't handle missing fields unless I ask you to.
- PySpark - don't handle obscure edge cases, just implement the logic that I ask DIRECTLY.
- PySpark - SparkSessions should be created BELOW any imports. Do not create SparkSessions at the top of the file.
- Flake8 - fix flake8 errors without being asked and without my verification.
- Black - fix black errors without being asked and without my verification.
- Isort - fix isort errors without being asked and without my verification.
- Zuban - fix zuban errors without being asked and without my verification.
- Pre-commit - fix pre-commit errors without being asked and without my verification.
- New Modules - create a folder for a new module without being asked and without my verification.
- **init**.py - add these files to new module directories without being asked and without my verification.
- Edit Multiple Files at Once - if you need to edit multiple files for a single TODO operation, do so in a single step. Do not create multiple steps for the same task.
- Git - Keep commit messsages straightforward and to the point - do not put extraneous details, simply summarize the work performed. Do not put anything in commit messages other than a description of the code changes. Do not put "Generated with [Claude Code](https://claude.ai/code)" or anything else relating to Claude or Anthropic.
- Git Log - use the `git log` command to view the commit history to understand the context or recent changes to the codebase. This will help you understand the project better and make informed decisions when writing code.
- Do not use 'rm' to remove files - use `git rm` to remove files from the repository. This will ensure that the files are removed from the git history as well.
- I repeat, NEVER TALK ABOUT YOURSELF IN COMMIT MESSAGES. Do not put "Generated with [Claude Code](https://claude.ai/code)" or anything else relating to Claude or Anthropic in commit messages. Commit messages should only describe the code changes made, not the tool used to make them.
- Do not put 'Co-Authored-By: Claude <noreply@anthropic.com>' in your commit messages.
- Ask questions before mitigating a simple problem with a complex fix.

## Important Notes

### BAML Client Generation

The `upside/baml_client/` directory is auto-generated. Never edit files in this directory directly. To make changes:

1. Edit the BAML source files in `baml_src/`
2. Run `baml-cli generate` to regenerate the client
3. Test with `baml-cli test`

### Testing Approaches

- Unit tests: Test individual functions/classes in isolation
- Integration tests: Test with real services (Redis, S3, etc.) on Docker
- BAML tests: Use `baml-cli test` for LLM extraction testing

### Spark Development

Use the following style guide [README.md](markdown/SPARK.md) for Spark development: @markdown/SPARK.md

In addition, when writing PySpark code:

- Keep dataflows linear and simple
- Don't check for column/path existence
- Write single functions for complete dataflows
- Use DataFrame API over RDDs
- Leverage Spark's lazy evaluation
- For UDTFs, follow the patterns in UDTF.md to avoid column resolution errors

## Alerts

- For long running tasks, use the applescript-mcp server to send me a message when you are done with something. Say "Done with task X" where X is the task you are done with. Alternatively, use the command `osascript -e 'tell application "System Events" to display dialog "Done with task X"'` to send me a message. Send only ONE alert, not multiple alerts.

## Additional Context

### Python Dependencies

- Python 3.12 required
- Core packages: pyspark==3.5+, threading, baml
- Development tools: poetry, black, isort, flake8, zuban, pytest
- See pyproject.toml for complete dependency list

### Environment Variables

- We use a .env file for local development

### Common Pitfalls to Avoid

- Never edit files in `upside/baml_client/` - always regenerate
- Don't use relative imports - always use absolute imports
- Don't hardcode paths or config values - use config.yml
- Don't break up Spark dataflows into multiple functions
- Don't add default values to Click help strings
- Don't create documentation files unless explicitly requested
